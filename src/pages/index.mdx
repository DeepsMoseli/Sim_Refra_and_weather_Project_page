---
layout: ../layouts/Layout.astro
title: Simulating Refractive Distortions & Weather-Induced Artifacts for Resource-Constrained Autonomous Perception
authors:
  - name: Moseli Mots’oehli
    institution: University of Hawaiʻi at Mānoa; MindForge AI / The Shard (South Africa)
    notes: ["*", "†"]
  - name: Feimei Chen
    institution: University of Hawaiʻi at Mānoa
    notes: ["*"]
  - name: Hok Wai Chan
    institution: University of Hawaiʻi at Mānoa
    notes: ["*"]
  - name: Itumeleng Tlali
    institution: MindForge AI (South Africa)
    notes: ["*"]
  - name: Thulani Babeli, FRM
    institution: MindForge AI (South Africa)
    notes: ["*"]

conference: ICCV 2025 — CV4DC Workshop (Honolulu, HI), October 19, 2025
notes:
  - symbol: "*"
    text: author note one
  - symbol: †
    text: author note two
links:
  - name: Paper
    url: https://arxiv.org/pdf/2507.05536
    icon: ri:file-pdf-2-line
  - name: Code
    url: https://github.com/DeepsMoseli/RefraWeather-Sim
    icon: ri:github-line
  - name: arXiv
    url: https://arxiv.org/pdf/2507.05536
    icon: academicons:arxiv

# The color theme of the page. Defaults to "device" (the preference set in the user's brower or operating system). Setting this to "light" or "dark" will override the user's preference. This is useful if your figures only look good in one theme.
theme: device

# This is the icon that appears in the user's browser tab. To customize, change the favicon.svg file in /public/, or add your own file to /public/ and change the filename here.
favicon: favicon.svg

# These keys are optional. If a link to your project page is in a Google search result, text message, or social media post, it will often appear as a "link preview card" based on its title, description, favicon, and thumbnail. After you publish your page, you can double check that these previews look right using [this tool](https://linkpreview.xyz/)
description: Simple project page template for your research paper, built with Astro and Tailwind CSS
thumbnail: screenshot-light.png
---

import Video from "../components/Video.astro";
import HighlightedSection from "../components/HighlightedSection.astro";
import SmallCaps from "../components/SmallCaps.astro";
import Figure from "../components/Figure.astro";
import Picture from "../components/Picture.astro";
import TwoColumns from "../components/TwoColumns.astro";
import YouTubeVideo from "../components/YouTubeVideo.astro";
import LaTeX from "../components/LaTeX.astro";
import { ThreeDimensional } from "../components/ThreeDimensional.tsx";
import { Comparison } from "../components/Comparison.tsx";

import outside from "../assets/outside.mp4";
import transformer from "../assets/transformer.webp";

import framework from "../assets/framework.png";
import flare_orig from "../assets/flare_orig.png";
import flare_sim from "../assets/flare_sim.png";
import uniform_fog_orig from "../assets/uniform_fog_orig.png";
import uniform_fog_sim from "../assets/uniform_fog_sim.png";
import all_geometric from "../assets/all_geometric.png";

import Table from "../components/Table.astro";
export const components = { table: Table }

<Video source={outside} />

<HighlightedSection>

## Abstract

The scarcity of autonomous vehicle datasets from developing regions, particularly across Africa’s diverse urban, rural, and unpaved roads, remains a key obstacle to robust
perception in low-resource settings. We present a procedural augmentation pipeline that enhances low-cost monocular dashcam footage with realistic refractive distortions and
weather-induced artifacts tailored to challenging African driving scenarios. Our refractive module simulates optical
effects from low-quality lenses and air turbulence, including lens distortion, Perlin noise, Thin-Plate Spline (TPS),and divergence-free (incompressible) warps. 
The weather module adds homogeneous fog, heterogeneous fog, and lens flare. To establish a benchmark, we provide baseline performance using three image restoration models. To support perception research in underrepresented African contexts,
without costly data collection, labeling, or simulation, we release our distortion toolkit, augmented dataset splits, and
benchmark results.

</HighlightedSection>

## Pipeline

<Figure>
  <Picture slot="figure" src={framework} alt="Diagram of the transformer deep learning architecture." invertInDarkMode />
  <Fragment slot="caption">Overview of our two-stage restoration pipeline: a Synthetic Distortion Module applies random refractive distortions and weather artifacts; a Refractive U-Net predicts a dense UV displacement field to correct warps; and a Deweathering U-Net removes weather artifacts. The two models are trained separately.</Fragment>
</Figure>


## Weather Artifacts

<Figure>
  <Comparison slot="figure" client:idle>
    <Picture slot="itemOne" src={uniform_fog_orig} alt="Original driving scene without fog effects" />
    <Picture slot="itemTwo" src={uniform_fog_sim} alt="Simulated driving scene with uniform fog applied" />
  </Comparison>
  <Fragment slot="caption">
    Comparison of a clear driving scene and its fog-augmented version generated using our uniform fog simulation. The method reduces visibility uniformly across depth.
  </Fragment>
</Figure>

<Figure>
  <Comparison slot="figure" client:idle>
    <Picture slot="itemOne" src={flare_orig} alt="Original driving scene without lens flare" />
    <Picture slot="itemTwo" src={flare_sim} alt="Simulated driving scene with lens flare artifacts" />
  </Comparison>
  <Fragment slot="caption">
    Comparison between a clear driving scene and its lens flare simulation. The flare overlay introduces realistic streaks and glare patterns to mimic sunlight reflections on the camera lens.
  </Fragment>
</Figure>



## Refractive Distortions

Our geometric distortion module simulates four physically inspired refractive effects commonly observed in real-world camera optics:

- **Radial distortion**: Introduces barrel- or pincushion-like bending caused by imperfect lens curvature.  
- **Perlin distortion**: Applies spatially varying local warps using Perlin noise to mimic glass irregularities or vibration-induced refractions.  
- **Thin-Plate Spline (TPS) distortion**: Produces smooth, non-linear deformations driven by sparse control points, resembling subtle structural warping.  
- **Divergence-Free distortion**: Generates fluid-like displacement fields constrained to preserve local area, creating realistic turbulence and heat-haze effects.

<Figure>
  <Picture slot="figure" src={all_geometric} alt="Examples of simulated refractive distortions including radial, Perlin, TPS, and divergence-free warps, with corresponding checkerboard and UV maps." />
  <Fragment slot="caption">
    Examples of simulated refractive distortions. Each row shows the undistorted input, the distorted output, a checkerboard visualization of geometric warping, and the corresponding UV map (R=U, G=V). Our framework models four refractive effects—radial, Perlin, TPS, and divergence-free—to produce diverse, physics-inspired geometric perturbations.
  </Fragment>
</Figure>


## Restoration Models & Evaluation

We fine-tune three pre-trained architectures for image restoration: **ResUNet** (ImageNet weights), **SegFormer-B1** (ImageNet weights), and a **conditional DDPM diffusion model** (`ddpm-cat-256` weights).
For **ResUNet** and **SegFormer-B1**, encoder weights are frozen and only decoder parameters are updated. The diffusion U-Net is fine-tuned end-to-end.
Each model is trained independently for two tasks:
- **Refractive undistortion** (correcting geometric warps)
- **Deweathering** (removing uniform fog, heterogeneous fog, and lens flare)

### Training Setup
- **Data split:** 80/20 random split for training and validation  
- **Epochs:** 35 for refractive undistortion; 15 for deweathering  
- **Batch sizes:** 16 (ResUNet, SegFormer-B1); 8 (DDPM)  
- **Optimizer:** AdamW  
- **Learning rates:** 5e-5 (ResUNet, SegFormer-B1); 1e-5 (DDPM)  
- **Schedulers:** ReduceLROnPlateau (factor 0.5, patience 5); cosine warm-up with 500 steps for DDPM  
- **Precision:** Mixed-precision AMP  
- **Image resolution:** 256×256 crops for DDPM

### Objective Functions

**Refractive undistortion**
- Supervised with ground-truth UV maps and clean RGB targets.
- Loss = L1(image reconstruction) + L1(UV flow):  
  difference between predicted clean image and ground truth **plus** difference between predicted UV flow and ground-truth UV flow.  
- The predicted clean image is obtained by warping the distorted input using the predicted UV flow, i.e., `pred_image = warp(x, y_hat_uv)`.

**Conditional DDPM**
- Loss = 0.8 × L1(noise prediction error) **plus** L2(image reconstruction) on `warp(x, y_hat)` vs the clean image.  
- The first term trains the UV-denoising U-Net; the second encourages the reconstructed image to match the ground truth.

**Deweathering**
- Loss = L1(image reconstruction) between the predicted deweathered image and the clean image.

### Evaluation Metrics
- **PSNR (Peak Signal-to-Noise Ratio):** computed from the mean squared error (MSE) between the restored image and ground truth; higher is better. We use MAX = 255 for 8-bit images.  
- **EPE (Endpoint Error):** average Euclidean distance per pixel between the predicted UV flow and the ground-truth UV flow; lower is better.


## Results

Validation metrics for the refractive **R** and weather-induced artifact **W** image restoration. Here, L is the combined weighted loss, L_rec the L1 reconstruction loss, L_k the L1 k-map/UV loss, PSNR, and EPE as described in Section 4.2.*

| Model          | L     | L_rec | L_k    | PSNR  | EPE  |
| :------------- | :---: | :---: | :----: | :---: | :--: |
| *W* ResUnet    | 0.065 | 0.059 | 0.0070 | 32.54 |  –   |
| *W* SegFormer  | 0.096 | 0.086 | 0.0100 | 28.99 |  –   |
| *W* DDPM       | 0.054 | 0.051 | 0.0065 | 30.80 |  –   |
| *R* ResUnet    | 0.034 | 0.029 | 0.0050 | 23.24 | 5.76 |
| *R* SegFormer  | 0.146 | 0.085 | 0.0550 | 16.29 | 69.82|
| *R* DDPM       | 0.044 | 0.039 | 0.0048 | 22.80 | 6.01 |


## BibTeX citation

Displaying your BibTeX citation in a code block makes it easy to copy and paste.

```bibtex
@article{motsoehli2025refraweather,
  title   = {Simulating Refractive Distortions and Weather-Induced Artifacts for Resource-Constrained Autonomous Perception},
  author  = {Mots'oehli, Moseli and Chen, Feimei and Chan, Hok Wai and Tlali, Itumeleng and Babeli, Thulani and Baek, Kyungim and Chen, Huaijin},
  journal = {arXiv preprint arXiv:2507.05536},
  year    = {2025}
}
```